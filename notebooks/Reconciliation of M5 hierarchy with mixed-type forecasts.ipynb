{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba38de137c74ddc",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This page reproduces the results as presented in *Probabilistic reconciliation of mixed-type hierarchical time series* (Zambon et al. 2024), published at UAI 2024 (the 40th Conference on Uncertainty in Artificial Intelligence).\n",
    "\n",
    "In particular, we replicate the reconciliation of the one-step ahead (h=1) forecasts of one store of the M5 competition (Makridakis, Spiliotis, and Assimakopoulos 2022). Sect. 5 of the paper presents the results for 10 stores, each reconciled 14 times using rolling one-step ahead forecasts.\n",
    "The original vignette containing the R counterpart of this page can be found [here](https://cran.r-project.org/web/packages/bayesRecon/vignettes/mixed_reconciliation.html). \n",
    "\n",
    "## Data and base forecasts\n",
    "\n",
    "The M5 competition (Makridakis, Spiliotis, and Assimakopoulos 2022) is about daily time series of sales data referring to 10 different stores. Each store has the same hierarchy: 3049 bottom time series (single items) and 11 upper time series, obtained by aggregating the items by department, product category, and store; see the figure below.\n",
    "\n",
    "![M5.png](../pictures/M5.png)\n",
    "\n",
    "We reproduce the results of the store “CA_1”. The base forecasts (for h=1) of the bottom and upper time series are stored in `M5_CA1_basefc` available as data in the original **‘[bayesRecon](https://cran.r-project.org/web/packages/bayesRecon/index.html)’** package in R. The base forecast are computed using ADAM (Svetunkov and Boylan 2023), implemented in the R package smooth (Svetunkov 2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f713d810682aa299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:36:31.851597Z",
     "start_time": "2024-11-11T16:36:31.844689Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from bayesreconpy.PMF import pmf_get_mean as PMF_get_mean\n",
    "from bayesreconpy.PMF import pmf_get_var as PMF_get_var\n",
    "from bayesreconpy.shrink_cov import schafer_strimmer_cov\n",
    "from bayesreconpy.reconc_gaussian import reconc_gaussian\n",
    "from bayesreconpy.reconc_MixCond import reconc_MixCond\n",
    "from bayesreconpy.reconc_TDcond import reconc_TDcond\n",
    "from bayesreconpy.utils import MVN_sample, samples_from_pmf\n",
    "from bayesreconpy.reconc_BUIS import reconc_BUIS\n",
    "\n",
    "M5_CA1_basefc = pd.read_pickle('../data/M5_CA1_basefc.pkl')\n",
    "\n",
    "# Hierarchy composed by 3060 time series: 3049 bottom and 11 upper\n",
    "n_b = 3049\n",
    "n_u = 11\n",
    "n = n_u + n_b\n",
    "\n",
    "#Load A matrix\n",
    "A = M5_CA1_basefc['A']\n",
    "\n",
    "# Load base forecasts\n",
    "base_fc_upper = M5_CA1_basefc['upper']\n",
    "base_fc_bottom = M5_CA1_basefc['bottom']\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "rec_fc = {\n",
    "    'Gauss': {},\n",
    "    'Mixed_cond': {},\n",
    "    'TD_cond': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6183ef8e-cad8-4874-af02-c01d0979ae72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:15:22.942441Z",
     "start_time": "2024-11-11T16:15:22.940469Z"
    }
   },
   "source": [
    "## Gaussian Reconciliation\n",
    "\n",
    "We first perform Gaussian reconciliation (`Gauss`, Corani et al. (2021)). It assumes all forecasts to be Gaussian, even though the bottom base forecasts are not Gaussian.\n",
    "\n",
    "We assume the upper base forecasts to be a multivariate Gaussian and we estimate their covariance matrix from the in-sample residuals. We assume also the bottom base forecasts to be independent Gaussians.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ef995c-48b8-4bae-bd6d-1b3b8e93d0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: CA_1, Residuals shape: (1941,)\n",
      "Name: HOBBIES, Residuals shape: (1941,)\n",
      "Name: HOUSEHOLD, Residuals shape: (1941,)\n",
      "Name: FOODS, Residuals shape: (1941,)\n",
      "Name: HOBBIES_1, Residuals shape: (1941,)\n",
      "Name: HOBBIES_2, Residuals shape: (1941,)\n",
      "Name: HOUSEHOLD_1, Residuals shape: (1941,)\n",
      "Name: HOUSEHOLD_2, Residuals shape: (1941,)\n",
      "Name: FOODS_1, Residuals shape: (1941,)\n",
      "Name: FOODS_2, Residuals shape: (1941,)\n",
      "Name: FOODS_3, Residuals shape: (1941,)\n"
     ]
    }
   ],
   "source": [
    "# Parameters of the upper base forecast distributions\n",
    "mu_u = {k:fc['mu'] for k, fc in base_fc_upper.items()} # upper means\n",
    "\n",
    "# Create a dictionary to store the names with their corresponding residuals\n",
    "residuals_dict = {fc: np.array(base_fc_upper[fc]['residuals']) for fc in base_fc_upper if 'residuals' in base_fc_upper[fc]}\n",
    "for name, residuals in residuals_dict.items():\n",
    "    print(f\"Name: {name}, Residuals shape: {residuals.shape}\")\n",
    "\n",
    "residuals_upper = np.vstack([residuals for residuals in residuals_dict.values()]).T\n",
    "\n",
    "# Compute the (shrinked) covariance matrix of the residuals\n",
    "\n",
    "Sigma_u = schafer_strimmer_cov(residuals_upper)['shrink_cov']  # Assuming a custom function for shrinkage\n",
    "Sigma_u = {\n",
    "    'names': list(residuals_dict.keys()),  # List of names corresponding to the diagonal elements\n",
    "    'Sigma_u': Sigma_u           # Covariance matrix\n",
    "}\n",
    "\n",
    "# Parameters of the bottom base forecast distributions\n",
    "mu_b = {}\n",
    "sd_b = {}\n",
    "\n",
    "# Loop through base_fc_bottom and calculate the mean and standard deviation for each pmf\n",
    "for k, fc in base_fc_bottom.items():\n",
    "    pmf = fc['pmf']  # Access 'pmf' inside each forecast entry\n",
    "\n",
    "    # Calculate the mean and standard deviation\n",
    "    mu_b_value = PMF_get_mean(pmf)\n",
    "    sd_b_value = PMF_get_var(pmf) ** 0.5\n",
    "\n",
    "    # Store the results in dictionaries with the key as the name\n",
    "    mu_b[k] = mu_b_value\n",
    "    sd_b[k] = sd_b_value\n",
    "\n",
    "# Create the covariance matrix (Sigma_b)\n",
    "Sigma_b = np.diag(np.array(list(sd_b.values())) ** 2)\n",
    "Sigma_b = {\n",
    "    'names': list(sd_b.keys()),  # List of names corresponding to the diagonal elements\n",
    "    'Sigma_b': Sigma_b           # Covariance matrix\n",
    "}\n",
    "\n",
    "# Mean and covariance matrix of the base forecasts\n",
    "base_forecasts_mu = {**mu_u, **mu_b}\n",
    "base_forecasts_Sigma = np.zeros((n, n))\n",
    "# Fill the upper-left block with Sigma_u\n",
    "base_forecasts_Sigma[:n_u, :n_u] = Sigma_u['Sigma_u']  # Upper block\n",
    "# Fill the bottom-right block with Sigma_b\n",
    "base_forecasts_Sigma[n_u:, n_u:] = Sigma_b['Sigma_b']  # Bottom block\n",
    "# Combine the names from both Sigma_u and Sigma_b\n",
    "combined_names = Sigma_u['names'] + Sigma_b['names']\n",
    "# Store the combined matrix and names in a dictionary\n",
    "base_forecasts_Sigma = {\n",
    "    'names': combined_names,       # Combined list of names\n",
    "    'Sigma': base_forecasts_Sigma  # Full covariance matrix\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b48a804-a2b8-46a7-acc9-8d11c8d121e5",
   "metadata": {},
   "source": [
    "We reconcile using the function `reconc_gaussian()`, which takes as input:\n",
    "\n",
    "- the summing matrix `A`;\n",
    "- the means of the base forecast, `base_forecasts_mu`;\n",
    "- the covariance of the base forecast, `base_forecasts_Sigma`.\n",
    "\n",
    "The function returns the reconciled mean and covariance for the bottom time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc8394c-5c69-464c-9832-6f559affcb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Gaussian reconciliation: 0.3 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "gauss = reconc_gaussian(A, list(base_forecasts_mu.values()),\n",
    "                        base_forecasts_Sigma['Sigma'])\n",
    "stop = time.time()\n",
    "\n",
    "# Create a dictionary for the reconciled forecasts, similar to rec_fc$Gauss in R\n",
    "rec_fc['Gauss'] = {\n",
    "    'mu_b': gauss['bottom_reconciled_mean'],            # Bottom-level reconciled mean\n",
    "    'Sigma_b': gauss['bottom_reconciled_covariance'],   # Bottom-level reconciled covariance\n",
    "    'mu_u': A @ gauss['bottom_reconciled_mean'],        # Upper-level reconciled mean\n",
    "    'Sigma_u': A @ gauss['bottom_reconciled_covariance'] @ A.T  # Upper-level reconciled covariance\n",
    "}\n",
    "\n",
    "# Calculate the time taken for reconciliation\n",
    "Gauss_time = round(stop - start, 2)\n",
    "\n",
    "# Output the time taken for reconciliation\n",
    "print(f\"Time taken by Gaussian reconciliation: {Gauss_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52972625-ae51-4485-be82-91c643cc9787",
   "metadata": {},
   "source": [
    "## Reconciliation with mixed-conditioning\n",
    "\n",
    "We now reconcile the forecasts using the mixed-conditioning approach of Zambon et al. (2024), Sect. 3. The algorithm is implemented in the function `reconc_MixCond()`. The function takes as input:\n",
    "\n",
    "- the aggregation matrix `A`;\n",
    "- the probability mass functions of the bottom base forecasts, stored in the list `fc_bottom_4rec`;\n",
    "- the parameters of the multivariate Gaussian distribution for the upper variables, `fc_upper_4rec`;\n",
    "- additional function parameters; among those note that `num_samples` specifies the number of samples used in the internal importance sampling (IS) algorithm.\n",
    "\n",
    "The function returns the reconciled forecasts in the form of probability mass functions for both the upper and bottom time series. The function parameter `return_type` can be changed to `samples` or `all` to obtain the IS samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4fbcf98-581c-43fb-bbaa-98a2ff725ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational time for Mix-cond reconciliation: 8.39 seconds\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "N_samples_IS = int(5e4)  # 50,000 samples\n",
    "\n",
    "# Base forecasts\n",
    "Sigma_u_np = np.array(Sigma_u['Sigma_u'])\n",
    "fc_upper_4rec = {'mu': mu_u, 'Sigma': Sigma_u_np}  # Dictionary for upper forecasts\n",
    "fc_bottom_4rec = {k: np.array(fc['pmf']) for k, fc in base_fc_bottom.items()}\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(seed)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Perform MixCond reconciliation\n",
    "mix_cond = reconc_MixCond(A, fc_bottom_4rec, fc_upper_4rec, bottom_in_type=\"pmf\",\n",
    "                          num_samples=N_samples_IS, return_type=\"pmf\", seed=seed)\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "rec_fc['Mixed_cond'] = {\n",
    "    'bottom': mix_cond['bottom_reconciled']['pmf'],  # Bottom-level reconciled PMFs\n",
    "    'upper': mix_cond['upper_reconciled']['pmf'],    # Upper-level reconciled PMFs\n",
    "    'ESS': mix_cond['ESS']                           # Effective Sample Size (ESS)\n",
    "}\n",
    "\n",
    "# Calculate the time taken for MixCond reconciliation\n",
    "MixCond_time = round(stop - start, 2)\n",
    "\n",
    "print(f\"Computational time for Mix-cond reconciliation: {MixCond_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f9a80-2214-41a2-a448-e39407ad2372",
   "metadata": {},
   "source": [
    "As discussed in Zambon et al. (2024), Sect. 3, conditioning with mixed variables performs poorly in high dimensions. This is because the bottom-up distribution, built by assuming the bottom forecasts to be independent, is untenable in high dimensions. Moreover, forecasts for count time series are usually biased and their sum tends to be strongly biased; see Zambon et al. (2024), Fig. 3, for a graphical example.\n",
    "\n",
    "## Top down conditioning\n",
    "\n",
    "Top down conditioning (TD-cond; see Zambon et al. (2024), Sect. 4) is a more reliable approach for reconciling mixed variables in high dimensions. The algorithm is implemented in the function `reconc_TDcond()`; it takes the same arguments as `reconc_MixCond()` and returns reconciled forecasts in the same format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fb1bbfc-f3b7-4138-9839-a834b0bae311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Only 99.6% of the upper samples are in the support of the bottom-up distribution; the others are discarded.\n"
     ]
    }
   ],
   "source": [
    "N_samples_TD = int(1e4)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Perform TD-cond reconciliation (assuming reconc_TDcond is implemented)\n",
    "# This will raise a warning if upper samples are discarded\n",
    "td = reconc_TDcond(A, fc_bottom_4rec, fc_upper_4rec,\n",
    "                   bottom_in_type=\"pmf\", num_samples=N_samples_TD,\n",
    "                   return_type=\"pmf\", seed=seed)\n",
    "#Warning: Only 99.6% of the upper samples are in the support of the \n",
    "#bottom-up distribution; the others are discarded.\n",
    "stop = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa2f00-9ee8-4874-b648-e17d0672ce9b",
   "metadata": {},
   "source": [
    "The algorithm TD-cond raises a warning regarding the incoherence between the joint bottom-up and the upper base forecasts. We will see that this warning does not impact the performances of TD-cond. An important note to be made here is, R and python uses different sampling schemes even with the same seed. So we might see some minor deviations of the results than the ones presented in R. But as we increase `N_samples_TD` , it becomes negligible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "becb8bb2-ab08-498f-84aa-9facbfccec8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational time for TD-cond reconciliation: 9.64 seconds\n"
     ]
    }
   ],
   "source": [
    "rec_fc['TD_cond'] = {\n",
    "    'bottom': td['bottom_reconciled']['pmf'],\n",
    "    'upper': td['upper_reconciled']['pmf']\n",
    "}\n",
    "\n",
    "TDCond_time = round(stop - start, 2)\n",
    "print(f\"Computational time for TD-cond reconciliation: {TDCond_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c39910-5236-43dc-beb2-a95f7981e033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
