{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6011eb426e796d46",
   "metadata": {},
   "source": [
    "# Properties of the reconciled distribution via conditioning\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This vignette reproduces the results of the paper *Properties of the reconciled distributions for Gaussian and count forecasts* (Zambon et al. 2024), accepted for publication in the International Journal of Forecasting. We replicate here the results obtained in the original [R package](https://cran.r-project.org/web/packages/bayesRecon/vignettes/reconciliation_properties.html).\n",
    "\n",
    "# Data and base forecasts\n",
    "\n",
    "The R package released a new data set, containing time series of counts of extreme market events in five economic sectors in the period 2005-2018 (3508 trading days). The counts are computed by considering 29 companies included in the Euro Stoxx 50 index and observing if the value of the CDS spread on a given day exceeds the 90-th percentile of its distribution in the last trading year. The companies are divided into the following sectors: Financial (FIN), Information and Communication Technology (ICT), Manufacturing (MFG), Energy (ENG), and Trade (TRD).\n",
    "\n",
    "The hierarchy is composed of 5 bottom time series, the daily number of extreme market events in each sector, and 1 upper time series (the sum of the different sectors). Data are stored in `extr_mkt_events.pkl`.\n",
    "![extr_mkt_events.jpeg](../pictures/extr_mkt_events.jpeg)\n",
    "\n",
    "The python counterpart of the base forecasts are stored in `extr_mkt_events_basefc.pkl` which are to be reconciled. They are produced using the model by (Agosto 2022); the predictive distributions are negative binomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "749510705029c15a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:15:12.093083Z",
     "start_time": "2024-11-12T10:15:12.091126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIf you want to run only N reconciliations (instead of 3508)\\nN = 200\\nactuals = actuals[:N,:]\\nbase_fc['mu'] = np.array(base_fc['mu'])[:N,:]\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy.stats import nbinom\n",
    "from numpy.random import default_rng\n",
    "from bayesreconpy.reconc_BUIS import reconc_BUIS\n",
    "\n",
    "n_b = 5\n",
    "n_u = 1\n",
    "n = n_b + n_u\n",
    "\n",
    "A = np.ones((n_u, n_b))\n",
    "\n",
    "# Actual values:\n",
    "actuals = pd.read_pickle('../data/extr_mkt_events.pkl')\n",
    "#Bse forecasts:\n",
    "base_fc = (pd.read_pickle('../data/extr_mkt_events_basefc.pkl'))\n",
    "\n",
    "N = actuals.shape[0]  # number of days (3508)\n",
    "\n",
    "\"\"\"\n",
    "If you want to run only N reconciliations (instead of 3508)\n",
    "N = 200\n",
    "actuals = actuals[:N,:]\n",
    "base_fc['mu'] = np.array(base_fc['mu'])[:N,:]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4212d828-24bc-46ef-9b2b-f8af55fd50b7",
   "metadata": {},
   "source": [
    "# Reconciliation via conditioning\n",
    "\n",
    "We reconcile the base forecasts via conditioning, using importance sampling. We use the `reconc_BUIS` function, which implements the BUIS algorithm (Zambon, Azzimonti, and Corani 2024); since there is only one upper time series in the hierarchy, the BUIS algorithm is equivalent to importance sampling. We perform 3508 reconciliations, one for each day, drawing each time 10,000 samples from the reconciled distribution. We use 10,000 samples instead of 100,000 (as in the paper) to speed up the computation.\n",
    "\n",
    "For each day, we save the empirical mean, median, and quantiles of the reconciled distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "643912cf-e59d-402a-bd44-64e0c93fd7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational time for 3508 reconciliations: 20.13 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize matrices to store mean, median, lower, and upper quantiles\n",
    "rec_means = np.full((N, n), np.nan)\n",
    "rec_medians = np.full((N, n), np.nan)\n",
    "rec_L = np.full((N, n), np.nan)\n",
    "rec_U = np.full((N, n), np.nan)\n",
    "\n",
    "# Interval coverage\n",
    "int_cov = 0.9\n",
    "q1 = (1 - int_cov) / 2\n",
    "q2 = (1 + int_cov) / 2\n",
    "\n",
    "# Number of samples for reconciled distribution\n",
    "N_samples = int(1e4)\n",
    "rng = default_rng(42)  # Seeded random number generator for reproducibility\n",
    "\n",
    "# Start timing\n",
    "start = time.time()\n",
    "\n",
    "for j in range(N):\n",
    "    # Prepare base forecasts for the current iteration\n",
    "    base_fc_j = []\n",
    "    for i in range(n):\n",
    "        # Fetch `size` and `mu` for each forecast from the corresponding DataFrame column and row\n",
    "        size_value = base_fc['size'].iloc[0, i]  # Use row 0 of `size` for all j's\n",
    "        mu_value = base_fc['mu'].iloc[j, i]\n",
    "        base_fc_j.append({\"size\": size_value, \"mu\": mu_value})\n",
    "\n",
    "    # Reconcile via importance sampling\n",
    "    buis = reconc_BUIS(A, base_fc_j, \"params\", \"nbinom\", num_samples=N_samples, seed=42)\n",
    "    samples_y = buis['reconciled_samples']\n",
    "\n",
    "    # Save mean, median, and quantiles\n",
    "    rec_means[j, :] = np.mean(samples_y, axis=1)  # Mean along rows\n",
    "    rec_medians[j, :] = np.median(samples_y, axis=1)  # Median along rows\n",
    "    rec_L[j, :] = np.quantile(samples_y, q1, axis=1)  # Lower quantile\n",
    "    rec_U[j, :] = np.quantile(samples_y, q2, axis=1)  # Upper quantile\n",
    "\n",
    "# End timing\n",
    "stop = time.time()\n",
    "\n",
    "# Print computation time\n",
    "print(f\"Computational time for {N} reconciliations: {round(stop - start, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70bb37a-4ded-4ada-b024-8c00bf85e18d",
   "metadata": {},
   "source": [
    "We compute the median and the quantiles of the negative binomial base forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec7325fd-387a-4496-993d-3f9bb75d14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize matrices\n",
    "base_means = base_fc['mu'].values  # Convert to numpy array for easier manipulation\n",
    "base_medians = np.full((N, n), np.nan)\n",
    "base_L = np.full((N, n), np.nan)\n",
    "base_U = np.full((N, n), np.nan)\n",
    "\n",
    "# Loop through each column (i.e., each forecast variable)\n",
    "for i in range(n):\n",
    "    size_value = base_fc['size'].iloc[0, i]  # Use row 0 of `size` for each i (if size is constant across rows)\n",
    "\n",
    "    # Calculate the median, lower, and upper quantiles for each value of `mu` in the column\n",
    "    base_medians[:, i] = [nbinom.ppf(0.5, size_value, 1 / (1 + mu)) for mu in base_means[:, i]]\n",
    "    base_L[:, i] = [nbinom.ppf(q1, size_value, 1 / (1 + mu)) for mu in base_means[:, i]]\n",
    "    base_U[:, i] = [nbinom.ppf(q2, size_value, 1 / (1 + mu)) for mu in base_means[:, i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9777ac5-0c8f-45ee-bb09-39ff284a6d94",
   "metadata": {},
   "source": [
    "For each day and for each time series, we compute the absolute error, the squared error, and the interval score for the base and reconciled forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e37226a-1783-4853-8e3c-3c60ef06a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the squared errors\n",
    "SE_base = (base_means - actuals) ** 2\n",
    "SE_rec = (rec_means - actuals) ** 2\n",
    "\n",
    "# Compute the absolute errors\n",
    "AE_base = np.abs(base_medians - actuals)\n",
    "AE_rec = np.abs(rec_medians - actuals)\n",
    "\n",
    "# Define the interval score function\n",
    "def interval_score(l, u, actual, int_cov=0.9):\n",
    "    is_score = (u - l) + \\\n",
    "               (2 / (1 - int_cov)) * (actual - u) * (actual > u) + \\\n",
    "               (2 / (1 - int_cov)) * (l - actual) * (l > actual)\n",
    "    return is_score\n",
    "\n",
    "# Vectorized computation of interval scores\n",
    "IS_base = np.vectorize(interval_score)(base_L, base_U, actuals)\n",
    "IS_rec = np.vectorize(interval_score)(rec_L, rec_U, actuals)\n",
    "\n",
    "# Reshape the results into N x n matrices if needed\n",
    "IS_base = IS_base.reshape(N, n)\n",
    "IS_rec = IS_rec.reshape(N, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5f6e3-a7ca-407e-b3d8-0022dd716c69",
   "metadata": {},
   "source": [
    "We compute and show the skill scores, which measure the improvement of the reconciled forecasts over the base forecasts. The skill score is symmetric and bounded between -2 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ee4bb3-d495-4db7-a67b-202c97e5a4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                |   ALL |   FIN |   ICT |   MFG |   ENG |   TRD |\n",
      "|:---------------|------:|------:|------:|------:|------:|------:|\n",
      "| Interval score |  1.36 |  1.41 |  1.67 |  1.95 |  1.72 |  1.75 |\n",
      "| Squared error  |  0.83 |  1.12 |  1.13 |  1.07 |  1.12 |  1.09 |\n",
      "| Absolute error |  0.73 |  0.25 |  0.49 |  1.96 |  0.5  |  0.61 |\n"
     ]
    }
   ],
   "source": [
    "# Compute skill scores (SS) for Absolute Error, Squared Error, and Interval Score\n",
    "SS_AE = (AE_base - AE_rec) / (AE_base + AE_rec) * 2\n",
    "SS_SE = (SE_base - SE_rec) / (SE_base + SE_rec) * 2\n",
    "SS_IS = (IS_base - IS_rec) / (IS_base + IS_rec) * 2\n",
    "\n",
    "# Replace NaN values with 0\n",
    "SS_AE = np.nan_to_num(SS_AE, nan=0)\n",
    "SS_SE = np.nan_to_num(SS_SE, nan=0)\n",
    "SS_IS = np.nan_to_num(SS_IS, nan=0)\n",
    "\n",
    "# Calculate column means for each skill score matrix\n",
    "mean_skill_scores = np.round([\n",
    "    SS_IS.mean(axis=0),\n",
    "    SS_SE.mean(axis=0),\n",
    "    SS_AE.mean(axis=0)\n",
    "], 2)\n",
    "\n",
    "# Convert to DataFrame and structure the output\n",
    "mean_skill_scores_df = pd.DataFrame(mean_skill_scores,\n",
    "                                    index=[\"Interval score\", \"Squared error\", \"Absolute error\"],\n",
    "                                    columns=actuals.columns if isinstance(actuals, pd.DataFrame) else range(n))\n",
    "\n",
    "# Display the results as a table\n",
    "print(mean_skill_scores_df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce07db0-f22a-49fc-a6ff-38143dc73882",
   "metadata": {},
   "source": [
    "The table closely matches Table 4 of the paper. In order to exactly reproduce the paper table, it is necessary increase the number of samples drawn from the reconciled distribution to 100,000.\n",
    "\n",
    "# Reconciled mean and variance\n",
    "\n",
    "We now show the effects of the reconciliation on the mean and variance of the forecast distribution. For more details, we refer to Section 3.2 of the paper.\n",
    "\n",
    "We observe two different behaviors for the reconciled upper mean: it can be between the base and the bottom-up mean (*combination* effect) or it can be lower than both (*concordant-shift* effect). We show them for two different days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4dd96b1-308e-444b-9675-bf0751d99f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   Base upper mean |   Bottom-up upper mean |   Reconciled upper mean |\n",
      "|---:|------------------:|-----------------------:|------------------------:|\n",
      "|  0 |             10.67 |                    9.1 |                    8.29 |\n"
     ]
    }
   ],
   "source": [
    "# Define the number of samples to draw\n",
    "N_samples = int(1e5)\n",
    "\n",
    "# Example of concordant-shift effect for j = 123 (124 in R)\n",
    "j = 123\n",
    "base_fc_j = []\n",
    "\n",
    "# Prepare base forecast for the specified index\n",
    "for i in range(n):\n",
    "    size_value = base_fc['size'].iloc[0, i]  # size is constant across rows\n",
    "    mu_value = base_fc['mu'].iloc[j, i]\n",
    "    base_fc_j.append({\"size\": size_value, \"mu\": mu_value})\n",
    "\n",
    "# Reconcile via importance sampling\n",
    "buis = reconc_BUIS(A, base_fc_j, \"params\", \"nbinom\", num_samples=N_samples, seed=42)\n",
    "samples_y = buis['reconciled_samples']\n",
    "\n",
    "# Compute the means\n",
    "base_upper_mean = round(base_fc['mu'].iloc[j, 0], 2)\n",
    "bottom_up_upper_mean = round(base_fc['mu'].iloc[j, 1:].sum(), 2)\n",
    "reconciled_upper_mean = round(np.mean(samples_y[0, :]), 2)\n",
    "\n",
    "# Display results in a structured format\n",
    "means = [base_upper_mean, bottom_up_upper_mean, reconciled_upper_mean]\n",
    "col_names = [\"Base upper mean\", \"Bottom-up upper mean\", \"Reconciled upper mean\"]\n",
    "\n",
    "# Create a DataFrame to display the results as a table\n",
    "means_df = pd.DataFrame([means], columns=col_names)\n",
    "print(means_df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f064a45-160d-4c8b-8ac8-3087a365bc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   Base upper mean |   Bottom-up upper mean |   Reconciled upper mean |\n",
      "|---:|------------------:|-----------------------:|------------------------:|\n",
      "|  0 |             26.51 |                  43.39 |                   35.82 |\n"
     ]
    }
   ],
   "source": [
    "j = 1699\n",
    "base_fc_j = []\n",
    "\n",
    "# Prepare base forecast for the specified index\n",
    "for i in range(n):\n",
    "    size_value = base_fc['size'].iloc[0, i]  # Assume size is constant across rows\n",
    "    mu_value = base_fc['mu'].iloc[j, i]\n",
    "    base_fc_j.append({\"size\": size_value, \"mu\": mu_value})\n",
    "\n",
    "# Reconcile via importance sampling\n",
    "buis = reconc_BUIS(A, base_fc_j, \"params\", \"nbinom\", num_samples=N_samples, seed=42)\n",
    "samples_y = buis['reconciled_samples']\n",
    "\n",
    "# Compute the means\n",
    "base_upper_mean = round(base_fc['mu'].iloc[j, 0], 2)\n",
    "bottom_up_upper_mean = round(base_fc['mu'].iloc[j, 1:].sum(), 2)\n",
    "reconciled_upper_mean = round(np.mean(samples_y[0, :]), 2)\n",
    "\n",
    "# Display results in a structured format\n",
    "means = [base_upper_mean, bottom_up_upper_mean, reconciled_upper_mean]\n",
    "col_names = [\"Base upper mean\", \"Bottom-up upper mean\", \"Reconciled upper mean\"]\n",
    "\n",
    "# Create a DataFrame to display the results as a table\n",
    "means_df = pd.DataFrame([means], columns=col_names)\n",
    "print(means_df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc20bdf7-f770-4640-8692-2fe73fa87be1",
   "metadata": {},
   "source": [
    "Finally, we show an example in which the variance of the bottom time series increases after reconciliation. This is a major difference with the Gaussian reconciliation, for which the reconciled variance is always smaller than the base variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd9f2b53-8bef-44e0-adb3-00dd471ec563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            |     0 |    1 |    2 |    3 |    4 |\n",
      "|:-----------|------:|-----:|-----:|-----:|-----:|\n",
      "| var base   | 11.43 | 2.23 | 1.48 | 0.34 | 1.38 |\n",
      "| var reconc | 14.05 | 2.57 | 1.64 | 0.38 | 1.53 |\n"
     ]
    }
   ],
   "source": [
    "j = 2307  \n",
    "\n",
    "# Prepare base forecast for the specified index\n",
    "base_fc_j = []\n",
    "for i in range(n):\n",
    "    size_value = base_fc['size'].iloc[0, i]  # Assume size is constant across rows\n",
    "    mu_value = base_fc['mu'].iloc[j, i]\n",
    "    base_fc_j.append({\"size\": size_value, \"mu\": mu_value})\n",
    "\n",
    "# Reconcile via importance sampling\n",
    "buis = reconc_BUIS(A, base_fc_j, \"params\", \"nbinom\", num_samples=N_samples, seed=42)\n",
    "samples_y = buis['reconciled_samples']\n",
    "\n",
    "# Compute variance of the base bottom forecasts\n",
    "base_bottom_var = [\n",
    "    np.var(np.random.negative_binomial(n=size, p=size / (size + mu), size=int(1e5)))\n",
    "    for mu, size in zip(base_fc['mu'].iloc[j, 1:], \n",
    "                        base_fc['size'].iloc[0, 1:])\n",
    "]\n",
    "\n",
    "# Compute variance of the reconciled bottom forecasts\n",
    "rec_bottom_var = np.var(samples_y[1:, :], axis=1)\n",
    "\n",
    "# Combine base and reconciled variances and display results\n",
    "bottom_var = np.vstack([base_bottom_var, rec_bottom_var])\n",
    "bottom_var_df = pd.DataFrame(bottom_var, index=[\"var base\", \"var reconc\"])\n",
    "\n",
    "# Display as a table with two decimal places\n",
    "print(bottom_var_df.round(2).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b3931a-ff4f-4a12-bf45-cb12ee22aae0",
   "metadata": {},
   "source": [
    "The results here match exactly with the ones available in the [original R vignette](https://cran.r-project.org/web/packages/bayesRecon/vignettes/reconciliation_properties.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
